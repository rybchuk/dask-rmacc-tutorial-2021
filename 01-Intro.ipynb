{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/dask-horizontal.svg' width=400>\n",
    "\n",
    "# Dask natively scales Python\n",
    "## Dask provides advanced parallelism for analytics, enabling performance at scale for the tools you love\n",
    "\n",
    "### Integrates with existing projects\n",
    "#### BUILT WITH THE BROADER COMMUNITY\n",
    "\n",
    "Dask is open source and freely available. It is developed in coordination with other community projects like Numpy, Pandas, and Scikit-Learn.\n",
    "\n",
    "*(from the Dask project homepage at dask.org)*\n",
    "\n",
    "* * * \n",
    "\n",
    "The notebooks used in this tutorial were developed by different members of the Dask community. In particular\n",
    "* [Notebooks 1-6](https://github.com/jacobtomlinson/dask-video-tutorial-2020) - Adam Breindel, Jacob Tomlinson (NVIDIA) \n",
    "* [Notebook 7](https://github.com/pangeo-gallery/osm2020tutorial) - Chelle Gentemann (Farallon Institute), Rich Signell (USGS), and Ryan Abernathey (LDEO) \n",
    "* These notebooks were lightly edited by Alex Rybchuk (CU Boulder) for presentation at RMACC 2021\n",
    "\n",
    "* * *\n",
    "\n",
    "__What Does This Mean?__\n",
    "* Built in Python\n",
    "* Scales *properly* from single laptops to 1000-node clusters\n",
    "* Leverages and interops with existing Python APIs as much as possible\n",
    "* Adheres to (Tim Peters') \"Zen of Python\" (https://www.python.org/dev/peps/pep-0020/) ... especially these elements:\n",
    "    * Explicit is better than implicit.\n",
    "    * Simple is better than complex.\n",
    "    * Complex is better than complicated.\n",
    "    * Readability counts. <i>[ed: that goes for docs, too!]</i>\n",
    "    * Special cases aren't special enough to break the rules.\n",
    "    * Although practicality beats purity.\n",
    "    * In the face of ambiguity, refuse the temptation to guess.\n",
    "    * If the implementation is hard to explain, it's a bad idea.\n",
    "    * If the implementation is easy to explain, it may be a good idea.\n",
    "* While we're borrowing inspiration, it Dask embodies one of Perl's slogans, making easy things easy and hard things possible\n",
    "    * Specifically, it supports common data-parallel abstractions like Pandas and Numpy\n",
    "    * But also allows scheduling arbitary custom computation that doesn't fit a preset mold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's See Some Code\n",
    "\n",
    "Before we go any further, let's take a look at one particular, common use case for Dask: scaling Pandas dataframes to \n",
    "* larger datasets (which don't fit in memory) and \n",
    "* multiple processes (which could be on multiple nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=2, threads_per_worker=1, memory_limit='1GB')\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe\n",
    "\n",
    "ddf = dask.dataframe.read_csv('data/beer_small.csv', blocksize=12e6)  # blocksize=12e6 is 12 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is this Dask Dataframe?\n",
    "\n",
    "A large, virtual dataframe divided along the index into multiple Pandas dataframes:\n",
    "\n",
    "<img src=\"images/dask-dataframe.svg\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.map_partitions(type).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipa = ddf[ddf.beer_style.str.contains('IPA')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipa.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ipa_review = ipa.groupby('brewery_name').review_overall.agg(['mean','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ipa_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ipa_review.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ipa_review.nlargest(20, 'mean').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compute` doesn't just run the work, it collects the result to a single, regular Pandas dataframe right here in our initial Python VM.\n",
    "\n",
    "Having a local result is convenient, but if we are generating large results, we may want (or need) to produce output in parallel to the filesystem, instead. \n",
    "\n",
    "There are writing counterparts to read methods which we can use:\n",
    "\n",
    "- `read_csv` \\ `to_csv`\n",
    "- `read_hdf` \\ `to_hdf`\n",
    "- `read_json` \\ `to_json`\n",
    "- `read_parquet` \\ `to_parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ipa_review.to_csv('ipa-*.csv') #the * is where the partition number will go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Dask\n",
    "\n",
    "Dask was created in 2014 as part of the Blase project, a DARPA funded project at Continuum/Anaconda. It has since grown into a multi-institution community project with developers from projects including NumPy, Pandas, Jupyter and Scikit-Learn. Many of the core Dask maintainers are employed to work on the project by companies including Continuum/Anaconda, Prefect, NVIDIA, Capital One, Saturn Cloud and Coiled.\n",
    "\n",
    "Fundamentally, Dask allows a variety of parallel workflows using existing Python constructs, patterns, or libraries, including dataframes, arrays (scaling out Numpy), bags (an unordered collection construct a bit like `Counter`), and `concurrent.futures`\n",
    "\n",
    "In addition to working in conjunction with Python ecosystem tools, Dask's extremely low scheduling overhead (nanoseconds in some cases) allows it work well even on single machines, and smoothly scale up.\n",
    "\n",
    "Dask supports a variety of use cases for industry and research: https://stories.dask.org/en/latest/\n",
    "\n",
    "With its recent 2.x releases, and integration to other projects (e.g., RAPIDS for GPU computation), many commercial enterprises are paying attention and jumping in to parallel Python with Dask.\n",
    "\n",
    "__Dask Ecosystem__\n",
    "\n",
    "In addition to the core Dask library and its Distributed scheduler, the Dask ecosystem connects several additional initiatives, including...\n",
    "* Dask ML - parallel machine learning, with a scikit-learn-style API\n",
    "* Dask-kubernetes\n",
    "* Dask-XGBoost\n",
    "* Dask-YARN\n",
    "* Dask-image\n",
    "* Dask-cuDF\n",
    "* ... and some others\n",
    "\n",
    "__What's Not Part of Dask?__\n",
    "\n",
    "There are lots of functions that integrate to Dask, but are not represented in the core Dask ecosystem, including...\n",
    "\n",
    "* a SQL engine\n",
    "* data storage\n",
    "* data catalog\n",
    "* visualization\n",
    "* coarse-grained scheduling / orchestration\n",
    "* streaming\n",
    "\n",
    "... although there are typically other Python packages that fill these needs (e.g., Kartothek or Intake for a data catalog).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Do We Set Up and/or Deploy Dask?\n",
    "\n",
    "The easiest way to install Dask is with Anaconda: `conda install dask`\n",
    "\n",
    "__Schedulers and Clustering__\n",
    "\n",
    "Dask has a simple default scheduler called the \"single machine scheduler\" -- this is the scheduler that's used if your `import dask` and start running code without explicitly using a `Client` object. It can be handy for quick-and-dirty testing, but I would suggest that a best practice is to __use the newer \"distributed scheduler\" even for single-machine workloads__\n",
    "\n",
    "The distributed scheduler can work with \n",
    "* threads in one process (although that is often not a great idea due to the GIL)\n",
    "* multiple processes on one machine\n",
    "* multiple processes on multiple machines\n",
    "\n",
    "The distributed scheduler has additional useful features including data locality awareness and realtime graphical dashboards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "Words that are used generally when talking about large-scale comuptations\n",
    "* **Parallel computing** - a computation where many calculations (processes) are being carried out at the same time\n",
    "* **Distributed computing** - a computing model that connects many machines (processors), each with their own memory, that are linked through a communication network\n",
    "* **Cluster** - A collection of nodes that shares computing power to complete processes\n",
    "* **Node** - Machines that are responsible for creating (computing), recieving, or transmitting info within the distributed system\n",
    "* **Process** - A thing that provides the resources to execute a program. By default, each process usually starts with one thread, but additional threads can be created. Some operating systems use the term \"task\" to refer to the program that is being executed. Several processes may be associated with the program. \n",
    "* **Thread** - An entity within a process that can be scheduled for execution. All threads within a process share its system resources. \n",
    "\n",
    "Words that are more closely tied to Dask\n",
    "* **Client** - The user-facing entry point where you write your Python code\n",
    "* **Scheduler** - This recieves tasks from the client. It then manages the flow of work and sends tasks to workers.\n",
    "* **Worker** - These compute the tasks that the scheduler assigns to them. They can also store and serve computed results for other workers (or Clients). \n",
    "\n",
    "Courtesy of [Coiled](https://coiled.io/scaling-data-science-in-python-with-dask/) and [StackOverflow](https://stackoverflow.com/questions/200469/what-is-the-difference-between-a-process-and-a-thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
