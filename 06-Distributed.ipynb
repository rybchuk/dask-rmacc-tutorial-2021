{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This notebook is being run by the presenter locally and will fail on Binder.\n",
    "\n",
    "# Distributed\n",
    "As we covered at the beginning Dask has the ability to run work on mulitple machines using the distributed scheduler.\n",
    "\n",
    "Until now we have actually been using the distributed scheduler for our work, but just on a single machine.\n",
    "\n",
    "When we instantiate a Client() object with no arguments it will attempt to locate a Dask cluster. It will check your local Dask config and environment variables to see if connection information has been specified. If not it will create an instance of LocalCluster and use that.\n",
    "\n",
    "Specifying connection information in config is useful for system administrators to provide access to their users. We do this in the Dask Helm Chart for Kubernetes, the chart installs a multi-node Dask cluster and a Jupyter server on a Kubernetes cluster and Jupyter is preconfigured to discover the distributed cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote clusters via SLURM\n",
    "A common way to distribute your work onto multiple machines is via an HPC System with SLURM. Dask has a cluster manager which handles nodes called `SLURMCluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to specify information about the nodes that you would like to use. Below, we specify information for the `blanca-lundquist` nodes on Blanca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up Cluster (for Blanca)\n",
    "## Node parameters\n",
    "##     I obtained this info from `scontrol show nodes`\n",
    "PARTITION = 'blanca-lundquist'  # aka \"queue\"\n",
    "ACCOUNT = 'blanca-lundquist'    # aka \"project\"\n",
    "NUMCORES = 40                   # Total number of cores\n",
    "TOTMEM = '100GB'    # Total memory; `FreeMem=100GB`, `RealMemory=185GB`\n",
    "# INTERFACE = 'ib0'\n",
    "WALLTIME = '12:00:00'\n",
    "TMPDIR = '/rc_scratch/olry2348/tmp'\n",
    "NUMPROCS = 1\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    queue=PARTITION,\n",
    "    project=ACCOUNT,\n",
    "    cores=NUMCORES,\n",
    "    processes=NUMPROCS,\n",
    "    memory=TOTMEM,\n",
    "    walltime=WALLTIME,\n",
    "    local_directory=TMPDIR)#,\n",
    "#     interface=INTERFACE)\n",
    "\n",
    "NUMNODES = 3\n",
    "cluster.scale(jobs=NUMNODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangeo",
   "language": "python",
   "name": "pangeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
